{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Información del curso-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figuras/banner_dl2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Librerías básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías básicas de esta notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Media pipe\n",
    "\n",
    "\n",
    "**MediaPipe** es una biblioteca de código abierto desarrollada por **Google** para implementar modelos de **visión por computadora y procesamiento de señales en tiempo real**. Está optimizada para funcionar en **móviles, web y escritorio**, con soporte para **GPU y CPU**.\n",
    "\n",
    "### Características principales  \n",
    "**Procesamiento en tiempo real**  \n",
    "**Compatible con múltiples plataformas** (Android, iOS, Web, Raspberry Pi, etc.)  \n",
    "**Modelos preentrenados listos para usar**  \n",
    "**Eficiente en dispositivos de baja potencia**  \n",
    "\n",
    "### Modelos disponibles (Pipelines)  \n",
    "\n",
    "| Modelo                  | Descripción |\n",
    "|-------------------------|------------|\n",
    "| **Face Mesh**           | Detección de **468 puntos clave del rostro** |\n",
    "| **Hands**               | Seguimiento de **21 puntos clave de las manos** |\n",
    "| **Pose**                | Estimación de **33 puntos clave del cuerpo** |\n",
    "| **Holistic**            | Combina Face Mesh, Hands y Pose para un **seguimiento corporal completo** |\n",
    "| **Objectron**           | Detección de **objetos en 3D** (como zapatos y sillas) |\n",
    "| **Selfie Segmentation** | Segmentación de fondo en imágenes y videos |\n",
    "| **Text**                | Reconocimiento de texto en imágenes |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FaceMesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758562119.390165   24208 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1758562119.391765   24779 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1758562119.395122   24772 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758562119.401796   24774 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 0 -> Cámara integrada\n",
    "# 1 -> Cámara USB\n",
    "# videos/emotions.mp4 -> Video de ejemplo\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "facemesh = mp.solutions.face_mesh\n",
    "face = facemesh.FaceMesh(static_image_mode=False, min_tracking_confidence=0.6,min_detection_confidence=0.6)\n",
    "draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    ret , frame = cap.read()\n",
    "    img=frame.copy()\n",
    "    #ajuste de tamaño a una fraccion del original\n",
    "    fraccion=1\n",
    "    image = cv2.resize(img,(0,0),fx=fraccion,fy=fraccion, interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    rgb = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    results = face.process(rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        for i in results.multi_face_landmarks:\n",
    "            draw.draw_landmarks(image,i,facemesh.FACEMESH_CONTOURS ,landmark_drawing_spec=draw.DrawingSpec(color=(255,0,0),circle_radius=1,thickness=1))\n",
    "    cv2.imshow('Face Mesh Window', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758562125.516209   24208 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1758562125.519001   24790 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1758562125.548723   24782 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758562125.574846   24786 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "drawing = mp.solutions.drawing_utils\n",
    "hands=mp.solutions.hands\n",
    "h=hands.Hands()\n",
    "\n",
    "# 0 -> Cámara integrada\n",
    "# 1 -> Cámara USB\n",
    "# videos/hands.mp4 -> Video de ejemplo\n",
    "cap=cv2.VideoCapture('videos/piedra_papel_tijera.mp4')\n",
    "\n",
    "while True:\n",
    "    ret , frame=cap.read()\n",
    "    img=frame.copy()\n",
    "    #ajuste de tamaño a una fraccion del original\n",
    "    fraccion=1\n",
    "    image = cv2.resize(img,(0,0),fx=fraccion,fy=fraccion, interpolation=cv2.INTER_NEAREST)\n",
    "    image = cv2.cvtColor(cv2.flip(image,1),cv2.COLOR_BGR2RGB)\n",
    "    results=h.process(image)\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for h_landmark in results.multi_hand_landmarks:\n",
    "            drawing.draw_landmarks(image,h_landmark,hands.HAND_CONNECTIONS,landmark_drawing_spec=drawing.DrawingSpec(color=(0,0,0),circle_radius=4,thickness=3),connection_drawing_spec=drawing.DrawingSpec(color=(255,255,255),thickness=5) )\n",
    "    cv2.imshow(\"Hand Tracking\",cv2.flip(image,1))\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pose Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758562130.754534   24208 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1758562130.756057   24809 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1758562130.809098   24802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758562130.832897   24804 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "drawing = mp.solutions.drawing_utils\n",
    "pose=mp.solutions.pose\n",
    "p=pose.Pose()\n",
    "\n",
    "# 0 -> Cámara integrada\n",
    "# 1 -> Cámara USB\n",
    "# videos/dance.mp4 -> Video de ejemplo\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret , frame=cap.read()\n",
    "    img=frame.copy()\n",
    "    #ajuste de tamaño a una fraccion del original\n",
    "    fraccion=1\n",
    "    image = cv2.resize(img,(0,0),fx=fraccion,fy=fraccion, interpolation=cv2.INTER_NEAREST)\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    results=p.process(image)\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    if results.pose_landmarks:\n",
    "        drawing.draw_landmarks(image,results.pose_landmarks,pose.POSE_CONNECTIONS)\n",
    "    cv2.imshow(\"POSE TRACKING\",image)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758560494.426928   24208 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1758560494.428543   24327 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1758560494.500624   24320 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758560494.530808   24323 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758560494.532778   24320 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758560494.534036   24322 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758560494.534307   24323 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758560494.546120   24322 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758560494.550744   24317 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1758560494.550916   24323 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "\n",
    "drawing = mp.solutions.drawing_utils\n",
    "holistic=mp.solutions.holistic\n",
    "h=holistic.Holistic(min_tracking_confidence=0.6, min_detection_confidence=0.6)\n",
    "\n",
    "# 0 -> Cámara integrada\n",
    "# 1 -> Cámara USB\n",
    "# videos/dance.mp4 -> Video de ejemplo\n",
    "\n",
    "cap=cv2.VideoCapture('videos/dance.mp4')\n",
    "\n",
    "while True:\n",
    "    ret , frame=cap.read()\n",
    "    frame = cv2.resize(frame, (1000, 800))\n",
    "    image = cv2.cvtColor(cv2.flip(frame,1),cv2.COLOR_BGR2RGB)\n",
    "    results=h.process(image)\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    drawing.draw_landmarks(image,results.face_landmarks,holistic.FACEMESH_CONTOURS)\n",
    "    drawing.draw_landmarks(image,results.left_hand_landmarks,holistic.HAND_CONNECTIONS)\n",
    "    drawing.draw_landmarks(image,results.right_hand_landmarks,holistic.HAND_CONNECTIONS)\n",
    "    drawing.draw_landmarks(image,results.pose_landmarks,holistic.POSE_CONNECTIONS)\n",
    "    cv2.imshow(\"Hand Tracking\", image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
